include 'dl/transformer/configs/autoregressive.gin'
include 'dl/transformer/configs/pos_emb_abs.gin'

model_name = 'gpt2-small'

batch_size = 32
max_seq_len = 1024
learning_rate = 1e-4
train_steps = 20_000
log_steps = 100
eval_interval = 100
eval_steps = 5
ckpt_steps = 1_000

GPT.n_layers = 12
GPT.dim = 768
GPT.max_seq_len = %max_seq_len

Attention.heads = 12
