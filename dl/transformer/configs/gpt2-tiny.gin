include 'dl/transformer/configs/autoregressive.gin'
include 'dl/transformer/configs/pos_emb_abs.gin'

model_name = 'gpt2-tiny'

batch_size = 32
max_seq_len = 128
learning_rate = 1e-4
train_steps = 100
log_steps = 10
eval_interval = 10
eval_steps = 5
ckpt_steps = 100

GPT.n_layers = 4
GPT.dim = 512
GPT.max_seq_len = %max_seq_len

Attention.heads = 2
