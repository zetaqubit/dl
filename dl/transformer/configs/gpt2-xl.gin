include 'dl/transformer/configs/autoregressive.gin'
include 'dl/transformer/configs/pos_emb_abs.gin'

model_name = 'gpt2-xl'

batch_size = 4
max_seq_len = 128
learning_rate = 5e-5
train_steps = 100_000
log_steps = 500
eval_interval = 1000
eval_steps = 1
ckpt_steps = 10_000

GPT.n_layers = 48
GPT.dim = 1600
GPT.max_seq_len = %max_seq_len

Attention.heads = 25
