model_name = 'gpt-8l-768d-128msl'

batch_size = 32
max_seq_len = 128
learning_rate = 1e-4
train_steps = 10_000
log_steps = 100
eval_interval = 1000
eval_steps = 10
ckpt_steps = 2_000

tokenizers.create.tok_type = 'gpt2'
tokenizers.create.max_seq_len = %max_seq_len

GPT.n_layers = 8
GPT.dim = 768
GPT.max_seq_len = %max_seq_len

AutoregressiveModel.net = @GPT

SelfAttention.heads = 8
