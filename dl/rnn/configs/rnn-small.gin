model_name = 'rnn-small'

batch_size = 32
max_seq_len = 128
learning_rate = 1e-4
train_steps = 100
log_steps = 10
eval_interval = 10
eval_steps = 1
ckpt_steps = 100

dim = 128

RNNCell.hidden_size = %dim
RNNCell.activation_hh = @torch.tanh
RNNCell.activation_hx = @torch.tanh

RnnLM.n_layers = 4
RnnLM.dim = %dim
RnnLM.max_seq_len = None

GenerativeRnnModel.net = @RnnLM()

dataset = 'enwik8'
tok_type = 'char'
RnnLM.vocab = 256
GenerativeRnnModel.ignore_index = 0

# tok_type = 'gpt2'
# RnnLM.vocab = 50257
# GenerativeRnnModel.ignore_index = 50256
